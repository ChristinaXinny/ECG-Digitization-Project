# é«˜çº§è®­ç»ƒåŠŸèƒ½æ–‡æ¡£

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜äº† ECG æ•°å­—åŒ–é¡¹ç›®ä¸­å®ç°çš„é«˜çº§è®­ç»ƒåŠŸèƒ½ï¼ŒåŒ…æ‹¬å­¦ä¹ ç‡è°ƒåº¦å™¨ã€è®­ç»ƒæ£€æŸ¥ç‚¹ã€æ··åˆç²¾åº¦è®­ç»ƒç­‰æŠ€æœ¯ç‰¹æ€§ã€‚

## ğŸ“‹ ç›®å½•

- [å­¦ä¹ ç‡è°ƒåº¦å™¨](#å­¦ä¹ ç‡è°ƒåº¦å™¨)
- [è®­ç»ƒæ£€æŸ¥ç‚¹](#è®­ç»ƒæ£€æŸ¥ç‚¹)
- [æ··åˆç²¾åº¦è®­ç»ƒ](#æ··åˆç²¾åº¦è®­ç»ƒ)
- [åˆ†å¸ƒå¼è®­ç»ƒ](#åˆ†å¸ƒå¼è®­ç»ƒ)
- [åŠŸèƒ½ä½¿ç”¨æŒ‡å—](#åŠŸèƒ½ä½¿ç”¨æŒ‡å—)
- [é…ç½®ç¤ºä¾‹](#é…ç½®ç¤ºä¾‹)
- [æ€§èƒ½ä¼˜åŒ–å»ºè®®](#æ€§èƒ½ä¼˜åŒ–å»ºè®®)

---

## ğŸ”„ å­¦ä¹ ç‡è°ƒåº¦å™¨ (Learning Rate Scheduler)

### æ¦‚è¿°

é¡¹ç›®å®ç°äº†å¤šç§ä¸»æµçš„å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥ï¼Œæ”¯æŒé…ç½®åŒ–çš„å­¦ä¹ ç‡è°ƒæ•´ï¼Œä»¥æé«˜æ¨¡å‹è®­ç»ƒçš„æ”¶æ•›æ€§å’Œæ€§èƒ½ã€‚

### æ”¯æŒçš„è°ƒåº¦å™¨

| è°ƒåº¦å™¨ç±»å‹ | å®ç°çŠ¶æ€ | æè¿° | é€‚ç”¨åœºæ™¯ |
|------------|----------|------|----------|
| **CosineAnnealingLR** | âœ… å·²å®ç° | ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦ | é•¿æœŸè®­ç»ƒï¼Œå¹³æ»‘è¡°å‡ |
| **StepLR** | âœ… å·²å®ç° | é˜¶æ¢¯å¼å­¦ä¹ ç‡è¡°å‡ | åˆ†é˜¶æ®µè®­ç»ƒ |
| **ReduceLROnPlateau** | âœ… å·²å®ç° | åŸºäºéªŒè¯æŒ‡æ ‡çš„è‡ªé€‚åº”è°ƒæ•´ | é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œè‡ªåŠ¨è°ƒæ•´ |
| **MultiStepLR** | âœ… å·²å®ç° | å¤šé‡Œç¨‹ç¢‘å¼è¡°å‡ | å¤æ‚è®­ç»ƒç­–ç•¥ |
| **None** | âœ… å·²å®ç° | å›ºå®šå­¦ä¹ ç‡ | ç®€å•è®­ç»ƒåœºæ™¯ |

### å®ç°ä½ç½®

- **æ ¸å¿ƒå®ç°**: [`engines/base_trainer.py:_setup_scheduler()`](../engines/base_trainer.py:132-174)
- **é…ç½®æ–‡ä»¶**: [`configs/base.yaml:SCHEDULER`](../configs/base.yaml:31-34)
- **è®­ç»ƒè°ƒç”¨**: [`engines/base_trainer.py:271-310`](../engines/base_trainer.py:271-310)

### ä½¿ç”¨ç¤ºä¾‹

```yaml
# configs/base.yaml
TRAIN:
  SCHEDULER:
    NAME: "CosineAnnealingLR"
    MIN_LR: 1e-6
    WARMUP_EPOCHS: 5
```

```python
# ä»£ç ä¸­ä½¿ç”¨
scheduler = optim.lr_scheduler.CosineAnnealingLR(
    optimizer,
    T_max=epochs,
    eta_min=min_lr
)

# è®­ç»ƒå¾ªç¯ä¸­
if scheduler is not None:
    scheduler.step()
```

---

## ğŸ’¾ è®­ç»ƒæ£€æŸ¥ç‚¹ (Training Checkpoints)

### æ¦‚è¿°

é¡¹ç›®å®ç°äº†å®Œæ•´çš„è®­ç»ƒæ£€æŸ¥ç‚¹ç®¡ç†ç³»ç»Ÿï¼Œæ”¯æŒæ¨¡å‹çŠ¶æ€ã€ä¼˜åŒ–å™¨çŠ¶æ€ã€è°ƒåº¦å™¨çŠ¶æ€å’Œè®­ç»ƒè¿›åº¦çš„ä¿å­˜ä¸æ¢å¤ã€‚

### åŠŸèƒ½ç‰¹æ€§

| åŠŸèƒ½ | å®ç°çŠ¶æ€ | æè¿° |
|------|----------|------|
| **æ¨¡å‹çŠ¶æ€ä¿å­˜** | âœ… å·²å®ç° | ä¿å­˜æ¨¡å‹æƒé‡å’Œåç½® |
| **ä¼˜åŒ–å™¨çŠ¶æ€** | âœ… å·²å®ç° | ä¿å­˜ä¼˜åŒ–å™¨çš„å†…éƒ¨çŠ¶æ€ |
| **è°ƒåº¦å™¨çŠ¶æ€** | âœ… å·²å®ç° | ä¿å­˜å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€ |
| **æ¢¯åº¦ç¼©æ”¾å™¨çŠ¶æ€** | âœ… å·²å®ç° | ä¿å­˜æ··åˆç²¾åº¦è®­ç»ƒçŠ¶æ€ |
| **è®­ç»ƒè¿›åº¦** | âœ… å·²å®ç° | ä¿å­˜å½“å‰epochã€global_stepç­‰ |
| **æœ€ä½³æ¨¡å‹ç›‘æ§** | âœ… å·²å®ç° | åŸºäºéªŒè¯æŒ‡æ ‡ä¿å­˜æœ€ä½³æ¨¡å‹ |
| **é…ç½®ä¿å­˜** | âœ… å·²å®ç° | ä¿å­˜è®­ç»ƒé…ç½®ä»¥ä¾›å¤ç° |

### å®ç°ä½ç½®

- **æ ¸å¿ƒå®ç°**: [`engines/base_trainer.py:404-474`](../engines/base_trainer.py:404-474)
- **é…ç½®æ–‡ä»¶**: [`configs/base.yaml:CHECKPOINT`](../configs/base.yaml:108-114)
- **ä¾èµ–æ¨¡å—**: `utils.checkpoint.CheckpointManager` (éœ€è¦å®ç°)

### æ£€æŸ¥ç‚¹ç»“æ„

```python
checkpoint_data = {
    'epoch': current_epoch,
    'global_step': global_step,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'scheduler_state_dict': scheduler.state_dict(),
    'scaler_state_dict': scaler.state_dict(),
    'best_metric': best_metric,
    'train_losses': train_losses,
    'val_losses': val_losses,
    'config': config
}
```

### ä½¿ç”¨ç¤ºä¾‹

```yaml
# configs/base.yaml
CHECKPOINT:
  SAVE_DIR: "outputs/checkpoints"
  SAVE_TOP_K: 3
  SAVE_LAST: True
  MONITOR: "val_loss"
  MODE: "min"
```

```python
# ä¿å­˜æ£€æŸ¥ç‚¹
trainer.save_checkpoint(epoch=100, metrics={'val_loss': 0.123})

# æ¢å¤è®­ç»ƒ
trainer = BaseTrainer(model, config, train_loader, val_loader,
                      resume_from="outputs/checkpoints/best_model.pth")
```

---

## âš¡ æ··åˆç²¾åº¦è®­ç»ƒ (Mixed Precision Training)

### æ¦‚è¿°

é¡¹ç›®æ”¯æŒè‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆAMPï¼‰ï¼Œä½¿ç”¨FP16è¿›è¡Œå‰å‘ä¼ æ’­å’Œæ¢¯åº¦è®¡ç®—ï¼Œä½¿ç”¨FP32è¿›è¡Œæƒé‡æ›´æ–°ï¼Œæ˜¾è‘—æå‡è®­ç»ƒé€Ÿåº¦å¹¶å‡å°‘æ˜¾å­˜å ç”¨ã€‚

### æŠ€æœ¯åŸç†

- **FP16å‰å‘ä¼ æ’­**: å‡å°‘è®¡ç®—é‡å’Œæ˜¾å­˜ä½¿ç”¨
- **æ¢¯åº¦ç¼©æ”¾**: é˜²æ­¢FP16ä¸‹æº¢é—®é¢˜
- **FP32æƒé‡æ›´æ–°**: ä¿æŒæ•°å€¼ç¨³å®šæ€§
- **åŠ¨æ€æŸå¤±ç¼©æ”¾**: è‡ªé€‚åº”è°ƒæ•´ç¼©æ”¾å› å­

### åŠŸèƒ½ç‰¹æ€§

| åŠŸèƒ½ | å®ç°çŠ¶æ€ | æè¿° |
|------|----------|------|
| **è‡ªåŠ¨æ··åˆç²¾åº¦** | âœ… å·²å®ç° | PyTorch AMPæ”¯æŒ |
| **æ¢¯åº¦ç¼©æ”¾** | âœ… å·²å®ç° | GradScaleré˜²æ­¢ä¸‹æº¢ |
| **é…ç½®æ§åˆ¶** | âœ… å·²å®ç° | å¯é€šè¿‡é…ç½®å¯ç”¨/ç¦ç”¨ |
| **è®¾å¤‡å…¼å®¹** | âœ… å·²å®ç° | è‡ªåŠ¨æ£€æµ‹CUDAæ”¯æŒ |

### å®ç°ä½ç½®

- **åˆå§‹åŒ–**: [`engines/base_trainer.py:53`](../engines/base_trainer.py:53)
- **è®­ç»ƒæ­¥éª¤**: [`engines/base_trainer.py:220-229`](../engines/base_trainer.py:220-229)
- **é…ç½®æ–‡ä»¶**: [`configs/base.yaml:DEVICE`](../configs/base.yaml:10-16)

### ä½¿ç”¨ç¤ºä¾‹

```yaml
# å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
DEVICE:
  MIXED_PRECISION: True
  AMP_ENABLED: True
```

```python
# ä»£ç å®ç°
self.scaler = torch.cuda.amp.GradScaler() if config.get('MIXED_PRECISION', True) else None

# è®­ç»ƒå¾ªç¯
if self.scaler:
    with torch.cuda.amp.autocast():
        loss, metrics = self._train_step(batch)

    self.optimizer.zero_grad()
    self.scaler.scale(loss).backward()
    self.scaler.step(self.optimizer)
    self.scaler.update()
else:
    # æ ‡å‡†ç²¾åº¦è®­ç»ƒ
    loss, metrics = self._train_step(batch)
    loss.backward()
    self.optimizer.step()
```

### æ€§èƒ½ä¼˜åŠ¿

| æŒ‡æ ‡ | æ ‡å‡†ç²¾åº¦ | æ··åˆç²¾åº¦ | æå‡å¹…åº¦ |
|------|----------|----------|----------|
| **è®­ç»ƒé€Ÿåº¦** | åŸºå‡† | +1.5-2.5x | æ˜¾è‘—æå‡ |
| **æ˜¾å­˜å ç”¨** | åŸºå‡† | -30-50% | å¤§å¹…å‡å°‘ |
| **æ•°å€¼ç¨³å®šæ€§** | é«˜ | é«˜ | æ— æŸå¤± |

---

## ğŸŒ åˆ†å¸ƒå¼è®­ç»ƒ (Distributed Training)

### å½“å‰çŠ¶æ€

âŒ **æœªå®ç°** - é¡¹ç›®ç›®å‰ä¸æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒåŠŸèƒ½ã€‚

### ç¼ºå¤±åŠŸèƒ½

| åŠŸèƒ½ | çŠ¶æ€ | ä¼˜å…ˆçº§ |
|------|------|--------|
| **DistributedDataParallel (DDP)** | âŒ æœªå®ç° | é«˜ |
| **å¤šGPUæ•°æ®å¹¶è¡Œ** | âŒ æœªå®ç° | é«˜ |
| **åˆ†å¸ƒå¼é‡‡æ ·å™¨** | âŒ æœªå®ç° | ä¸­ |
| **è¿›ç¨‹ç»„åˆå§‹åŒ–** | âŒ æœªå®ç° | é«˜ |
| **èŠ‚ç‚¹é—´é€šä¿¡** | âŒ æœªå®ç° | ä¸­ |

### å®ç°å»ºè®®

å¦‚éœ€è¦æ·»åŠ åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒï¼Œå»ºè®®å®ç°ä»¥ä¸‹ç»„ä»¶ï¼š

1. **åˆ†å¸ƒå¼åˆå§‹åŒ–**
   ```python
   import torch.distributed as dist

   def setup_distributed(rank, world_size):
       dist.init_process_group(
           backend='nccl',
           init_method='env://',
           world_size=world_size,
           rank=rank
       )
   ```

2. **DDPæ¨¡å‹åŒ…è£…**
   ```python
   model = torch.nn.parallel.DistributedDataParallel(
       model, device_ids=[local_rank]
   )
   ```

3. **åˆ†å¸ƒå¼é‡‡æ ·å™¨**
   ```python
   sampler = torch.utils.data.distributed.DistributedSampler(
       dataset, num_replicas=world_size, rank=rank
   )
   ```

---

## ğŸ› ï¸ åŠŸèƒ½ä½¿ç”¨æŒ‡å—

### å¿«é€Ÿå¯ç”¨é«˜çº§åŠŸèƒ½

1. **é…ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨**
   ```yaml
   TRAIN:
     SCHEDULER:
       NAME: "CosineAnnealingLR"
       MIN_LR: 1e-6
   ```

2. **å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ**
   ```yaml
   DEVICE:
     MIXED_PRECISION: True
   ```

3. **é…ç½®æ£€æŸ¥ç‚¹ä¿å­˜**
   ```yaml
   CHECKPOINT:
     SAVE_TOP_K: 3
     MONITOR: "val_loss"
   ```

### è®­ç»ƒå‘½ä»¤ç¤ºä¾‹

```bash
# æ ‡å‡†è®­ç»ƒ
python train.py --config configs/base.yaml

# å¯ç”¨æ··åˆç²¾åº¦çš„è®­ç»ƒ
python train.py --config configs/base.yaml --mixed-precision

# ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
python train.py --config configs/base.yaml --resume outputs/checkpoints/latest.pth

# å¤šGPUè®­ç»ƒï¼ˆéœ€è¦å…ˆå®ç°åˆ†å¸ƒå¼åŠŸèƒ½ï¼‰
# python -m torch.distributed.launch --nproc_per_node=4 train.py
```

---

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–å»ºè®®

### å†…å­˜ä¼˜åŒ–

1. **å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ**
   - å‡å°‘æ˜¾å­˜å ç”¨ 30-50%
   - æå‡è®­ç»ƒé€Ÿåº¦ 1.5-2.5x

2. **æ¢¯åº¦ç´¯ç§¯**
   ```python
   # åœ¨é…ç½®ä¸­è®¾ç½®
   TRAIN:
     GRADIENT_ACCUMULATION_STEPS: 4
   ```

3. **æ‰¹æ¬¡å¤§å°è°ƒä¼˜**
   - æ ¹æ® GPU æ˜¾å­˜è°ƒæ•´ batch_size
   - ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯æ¨¡æ‹Ÿå¤§æ‰¹æ¬¡è®­ç»ƒ

### è®­ç»ƒé€Ÿåº¦ä¼˜åŒ–

1. **å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥é€‰æ‹©**
   - **CosineAnnealingLR**: é€‚åˆé•¿æœŸè®­ç»ƒ
   - **ReduceLROnPlateau**: é€‚åˆè‡ªåŠ¨è°ƒä¼˜
   - **StepLR**: é€‚åˆåˆ†é˜¶æ®µè®­ç»ƒ

2. **æ•°æ®åŠ è½½ä¼˜åŒ–**
   ```yaml
   DEVICE:
     NUM_WORKERS: 4
     PIN_MEMORY: True
   ```

3. **æ£€æŸ¥ç‚¹é¢‘ç‡ä¼˜åŒ–**
   - é¢‘ç¹ä¿å­˜ä¼šå½±å“è®­ç»ƒé€Ÿåº¦
   - å»ºè®®æ¯ 5-10 ä¸ª epoch ä¿å­˜ä¸€æ¬¡

### æ•°å€¼ç¨³å®šæ€§

1. **æ¢¯åº¦è£å‰ª**
   ```yaml
   TRAIN:
     GRADIENT_CLIP: 1.0
   ```

2. **æƒé‡åˆå§‹åŒ–**
   - ä½¿ç”¨é€‚å½“çš„æƒé‡åˆå§‹åŒ–ç­–ç•¥
   - è€ƒè™‘ä½¿ç”¨é¢„è®­ç»ƒæƒé‡

3. **å­¦ä¹ ç‡èŒƒå›´**
   - åˆå§‹å­¦ä¹ ç‡: 1e-4 åˆ° 1e-3
   - æœ€å°å­¦ä¹ ç‡: 1e-6 åˆ° 1e-7

---

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **æ··åˆç²¾åº¦è®­ç»ƒæŠ¥é”™**
   ```bash
   # ç¡®ä¿CUDAç‰ˆæœ¬æ”¯æŒAMP
   python -c "import torch; print(torch.cuda.amp.is_available())"
   ```

2. **æ£€æŸ¥ç‚¹åŠ è½½å¤±è´¥**
   ```python
   # æ£€æŸ¥æ£€æŸ¥ç‚¹æ–‡ä»¶å®Œæ•´æ€§
   import torch
   checkpoint = torch.load('path/to/checkpoint.pth')
   print(checkpoint.keys())
   ```

3. **å­¦ä¹ ç‡è°ƒåº¦å™¨ä¸å·¥ä½œ**
   ```python
   # ç¡®ä¿åœ¨è®­ç»ƒå¾ªç¯ä¸­è°ƒç”¨scheduler.step()
   scheduler.step()  # åœ¨æ¯ä¸ªepochç»“æŸåè°ƒç”¨
   ```

### æ€§èƒ½ç›‘æ§

1. **æ˜¾å­˜ç›‘æ§**
   ```python
   import torch
   print(f"Allocated: {torch.cuda.memory_allocated()/1024**3:.2f}GB")
   print(f"Cached: {torch.cuda.memory_reserved()/1024**3:.2f}GB")
   ```

2. **è®­ç»ƒé€Ÿåº¦ç›‘æ§**
   ```python
   import time
   start_time = time.time()
   # ... training step ...
   step_time = time.time() - start_time
   print(f"Step time: {step_time:.3f}s")
   ```

---

## ğŸ“š ç›¸å…³èµ„æº

- [PyTorch AMP å®˜æ–¹æ–‡æ¡£](https://pytorch.org/docs/stable/amp.html)
- [PyTorch åˆ†å¸ƒå¼è®­ç»ƒæŒ‡å—](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)
- [å­¦ä¹ ç‡è°ƒåº¦å™¨è¯¦è§£](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate)

---

## ğŸ“ æ›´æ–°æ—¥å¿—

| ç‰ˆæœ¬ | æ—¥æœŸ | æ›´æ–°å†…å®¹ |
|------|------|----------|
| 1.0.0 | 2024-11-18 | åˆå§‹æ–‡æ¡£ï¼ŒåŒ…å«ç°æœ‰åŠŸèƒ½è¯´æ˜ |
| 1.0.1 | 2024-11-18 | æ·»åŠ æ€§èƒ½ä¼˜åŒ–å»ºè®®å’Œæ•…éšœæ’é™¤æŒ‡å— |

---

*æœ¬æ–‡æ¡£éšé¡¹ç›®æ›´æ–°è€Œç»´æŠ¤ï¼Œå¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·æäº¤ Issue æˆ– Pull Requestã€‚*