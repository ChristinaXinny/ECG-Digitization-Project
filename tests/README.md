# Test Suite for ECG Digitization Project

This directory contains comprehensive tests for the ECG Digitization Project, covering all components from data loading to model training and inference.

## Test Structure

```
tests/
â”œâ”€â”€ __init__.py              # Package initialization
â”œâ”€â”€ README.md               # This file
â”œâ”€â”€ run_tests.py            # Main test runner with options
â”œâ”€â”€ test_suite.py           # Comprehensive test suite
â”œâ”€â”€ test_data_pipeline.py   # Data loading and preprocessing tests
â”œâ”€â”€ test_models.py          # Model architecture and component tests
â””â”€â”€ test_training.py        # Training and inference tests
```

## Test Categories

### 1. Unit Tests
- **Configuration Loading**: Test config file parsing and merging
- **Model Components**: Test individual model heads and components
- **Loss Functions**: Test loss function implementations
- **Metrics**: Test metric calculations

### 2. Integration Tests
- **Data Pipeline**: Test data loading and preprocessing
- **Model Integration**: Test complete model forward pass
- **Training Engine**: Test trainer functionality
- **Inference Engine**: Test inference pipeline

### 3. System Tests
- **End-to-End Training**: Test complete training workflow
- **Ablation Framework**: Test ablation study framework
- **Performance**: Test model performance and resource usage

## Running Tests

### Quick Start
```bash
# Run all tests
python tests/run_tests.py

# Run with verbose output
python tests/run_tests.py --verbose

# Run quick tests only
python tests/run_tests.py --quick
```

### Specific Tests
```bash
# Run specific test class
python tests/run_tests.py --tests TestStage0Model

# Run specific test method
python tests/run_tests.py --tests TestStage0Model.test_model_creation

# Run multiple specific tests
python tests/run_tests.py --tests TestStage0Model TestDataModule
```

### Performance Tests
```bash
# Run performance-related tests
python tests/run_tests.py --performance

# Skip slow tests for faster execution
python tests/run_tests.py --skip-slow
```

### Individual Test Files
```bash
# Run specific test file
python -m unittest tests.test_models

# Run specific test class from file
python -m unittest tests.test_models.TestDetectionHeads

# Run specific test method
python -m unittest tests.test_models.TestDetectionHeads.test_basic_detection_head
```

## Test Coverage

### Data Pipeline Tests (`test_data_pipeline.py`)
- âœ… Data augmentation strategies
- âœ… Dataset loading and validation
- âœ… Data module setup
- âœ… Corrupted data handling
- âœ… Transform consistency

### Model Tests (`test_models.py`)
- âœ… Detection heads (basic, multi-scale, attention)
- âœ… Segmentation heads
- âœ… Regression heads
- âœ… Classification heads
- âœ… Stage0Net model
- âœ… Gradient flow validation
- âœ… Configuration variations
- âœ… Different input sizes

### Training Tests (`test_training.py`)
- âœ… Loss function implementations
- âœ… Metrics calculations
- âœ… Trainer functionality
- âœ… Inference engine
- âœ… Checkpoint saving/loading
- âœ… Device selection (CPU/GPU)
- âœ… End-to-end training pipeline

### Integration Tests (`test_suite.py`)
- âœ… Configuration loading and merging
- âœ… Data module integration
- âœ… Model component integration
- âœ… Training engine integration
- âœ… Ablation framework integration
- âœ… Complete pipeline testing

## Test Configuration

Tests use minimal fake data to ensure fast execution and avoid dependencies on real ECG data. All test data is created in temporary directories and cleaned up automatically.

### GPU Testing
Tests will automatically use GPU if available. To force CPU testing:
```bash
CUDA_VISIBLE_DEVICES="" python tests/run_tests.py
```

### Verbose Output
Enable detailed test output:
```bash
python tests/run_tests.py --verbose
```

## Expected Test Results

When all tests pass, you should see output similar to:
```
ğŸš€ Starting ECG Digitization Test Suite
Verbose: False
Skip slow tests: False

============================================================
Running TestConfigLoading
============================================================
âœ… TestConfigLoading - PASSED (0.12s)

...

============================================================
FINAL TEST SUMMARY
============================================================
Total tests: 45
Passed: 45 âœ…
Failed: 0 âŒ
Total duration: 12.34s

Success rate: 100.0%

ğŸ‰ All tests passed!
```

## Troubleshooting

### Common Issues

1. **Import Errors**
   ```
   ModuleNotFoundError: No module named 'models'
   ```
   **Solution**: Ensure you're running tests from the project root directory.

2. **CUDA Out of Memory**
   ```
   RuntimeError: CUDA out of memory
   ```
   **Solution**: Run tests on CPU or use `CUDA_VISIBLE_DEVICES=""`.

3. **Permission Errors**
   ```
   PermissionError: [Errno 13] Permission denied
   ```
   **Solution**: Check file permissions for the project directory.

4. **Missing Dependencies**
   ```
   ImportError: No module named 'timm'
   ```
   **Solution**: Install missing dependencies with `pip install -r requirements.txt`.

### Debugging Failed Tests

1. **Enable Verbose Output**:
   ```bash
   python tests/run_tests.py --verbose --tests <failing_test>
   ```

2. **Run Individual Test Method**:
   ```bash
   python -m unittest tests.test_models.TestStage0Model.test_model_creation
   ```

3. **Check Test Output**:
   Tests capture and display detailed error messages when verbose mode is enabled.

## Writing New Tests

### Test Structure
```python
import unittest
import torch

class NewTest(unittest.TestCase):
    def setUp(self):
        """Setup test fixtures."""
        pass

    def tearDown(self):
        """Clean up test fixtures."""
        pass

    def test_new_functionality(self):
        """Test new functionality."""
        # Test implementation
        self.assertTrue(condition)
```

### Best Practices
1. **Use descriptive test names** that clearly indicate what is being tested
2. **Test both success and failure cases** when applicable
3. **Use setUp/tearDown** for common test setup and cleanup
4. **Mock external dependencies** to isolate tests
5. **Keep tests fast** by using minimal fake data
6. **Test edge cases** like empty inputs, invalid data, etc.

### Adding Tests to the Runner
To include new test classes in the main test runner, add them to the appropriate category in `run_tests.py`:

```python
unit_tests = [
    TestConfigLoading,
    NewTestClass,  # Add new test class here
    # ...
]
```

## Continuous Integration

These tests are designed to be run in CI/CD pipelines:

```yaml
# Example GitHub Actions workflow
- name: Run Tests
  run: |
    python tests/run_tests.py --verbose
```

The tests are:
- âœ… Fast (complete suite runs in < 2 minutes)
- âœ… Isolated (no external dependencies)
- âœ… Deterministic (consistent results)
- âœ… Comprehensive (good coverage)

## Performance Benchmarks

Test execution times on typical hardware:
- **Quick tests**: ~5 seconds
- **Unit tests**: ~30 seconds
- **Integration tests**: ~60 seconds
- **All tests**: ~120 seconds

These benchmarks help identify performance regressions and ensure tests remain fast enough for frequent execution.